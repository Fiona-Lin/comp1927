data 1 = 1000 ordered
data 3 = 100 000 ordered
For data sets 1 and 3 


These groups of numbers are arranged in ascending order. Therefore, the program only needs to compare each number with the pivot, not swap. The complexity of this operation is O(n-1). After this we divide the data into two sub-arrays passed to the recursive call. This process will make log n calls until the array's length is 1 in random data set. This results in a complexity for compare and recursive call operation as  O( N log N). However , in this case, because there is no swap, n-1 calls are made; the partition always returns the rightmost element back as the pivot.  This results in a complexity of  O ( N^2) for the comparative and recursive call operations. As it is not able to divide the array into two subsets, this makes a native strategy slower than other two . Thus those optimised approaches' performance dramatic improved for larger size data sets because we no longer choose the rightmost pivot, we spread the pivot in the middle or randomly pick in the array, the sorting gets sped up.


data 2 = 1000 reversed
data 4 = 100 000 reversed
These groups of numbers are arranged in descending order. Therefore, the program needs to compare each number with the pivot and do a swap for every number. The complexity of this is O(n-1) to iterate through and (n-1) to swap. Combined, these two operations result in a quadratic complexity - O(n^ 2). Complexity is calculated using the dominant operation (largest term), hence the recursive call can be ignored since its complexity only O(log N).


Because it is reversed, the function compares the minimum at the rightmost position and swaps all the other numbers to the right side of this pivot. Now the pivot is in the leftmost position after all the swaps and the partition will always return the pivot index back. This will not divide the array into two subsets and makes native approach slower than the other two (resulting in n - 1 calls like above). The optimised methods' comparative performance drastically improved for larger size data sets. This is because we no longer choose the rightmost pivot, we choose the middle as a pivot or randomly pick an element from the array - reducing the likelihood of choosing a ‘bad’ pivot (one with a highly uneven split). Therefore the sorting gets sped up. This is because splitting the array in two allows us to sort each side separately, half the time saving us from iterating through the entire array.




For a size of 100 000 reversed (data set 4), the naive pivot strategy was by far the most time consuming; taking 13 seconds compared to 0.056s for the randomised and 0.048s for the median of 3 partitioning. This trend continues throughout the vast majority of data sets, making the naive quicksort the poorest performing of the 3 algorithms. In some of the data sets randomised pivot was slower, meaning that the median of 3 partition pivot seemed to be the most efficient for the worst case of each. The more randomised the approach, the better the worst case became. The native sort is the worst as you are required to compare n times and swap n - 1 times which results in an O(n^2) complexity, the exponential obviously being the least efficient. For the randomised and median, because the pivot is not the rightmost - saves some swaps, it is not the minimum of the data set and therefore will perform better than the worst case.



data 5 = 100 randomised (w duplicates)
data 6 = 1000 randomised (w duplicates) 1-100


data 7 =  1000 randomised (w duplicates) 27-9997
data 8 =  10 000 randomised (w duplicates) - 10000 - 10000




With the randomised data sets, the pivot will no longer be in the leftmost or the rightmost. It is spread out, requiring the pivot  to only make two recursive calls to sort the two divided sub-arrays. This sorting process' complexity averages to O(N log N).




[a]should this part be at the beginning? ?
